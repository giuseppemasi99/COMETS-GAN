{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"../data/\"\n",
    "!ls $DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA + \"ohlc_KO_PEP_NVDA_KSU_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "\n",
    "def is_fitted(scaler: Union[MinMaxScaler, StandardScaler]) -> bool:\n",
    "    try:\n",
    "        check_is_fitted(scaler)\n",
    "        return True\n",
    "    except NotFittedError:\n",
    "        return False\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame, targets: List[str]) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def inverse_transform(self, x: np.ndarray, x_last: Optional[np.ndarray]) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__} (scaler={self.scaler})\"\n",
    "\n",
    "\n",
    "class ScalerPipeline(Pipeline):\n",
    "    def __init__(self, scaler: Union[MinMaxScaler, StandardScaler]) -> None:\n",
    "        super(ScalerPipeline, self).__init__()\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame, targets: List[str]) -> np.ndarray:\n",
    "        df_targets = df[targets]\n",
    "\n",
    "        if not is_fitted(self.scaler):\n",
    "            self.scaler.fit(df_targets)\n",
    "\n",
    "        return self.scaler.transform(df_targets)\n",
    "\n",
    "    def inverse_transform(self, x: np.ndarray, x_last: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        return self.scaler.inverse_transform(x)\n",
    "\n",
    "\n",
    "class LogReturnPipeline(Pipeline):\n",
    "    def __init__(self, scaler: Union[MinMaxScaler, StandardScaler]) -> None:\n",
    "        super(LogReturnPipeline, self).__init__()\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame, targets: List[str]) -> np.ndarray:\n",
    "        df_targets = df[targets]\n",
    "        log_returns = np.log(df_targets / df_targets.shift(1)).fillna(0).to_numpy()\n",
    "\n",
    "        if not is_fitted(self.scaler):\n",
    "            self.scaler.fit(log_returns)\n",
    "\n",
    "        return self.scaler.transform(log_returns)\n",
    "\n",
    "    def inverse_transform(self, x: np.ndarray, x_last: np.ndarray) -> np.ndarray:\n",
    "        log_returns = self.scaler.inverse_transform(x)\n",
    "        return x_last * (np.cumprod(np.exp(log_returns), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_holder_exponent(x, delta, q=1, k=2):\n",
    "    sqrt_pi = np.sqrt(np.pi)\n",
    "    K = np.var(x)\n",
    "    x = np.log(x + 1)\n",
    "    H = list()\n",
    "    for t in range(delta, len(x), delta):\n",
    "        S = 0\n",
    "        for j in range(t - delta, t - q):\n",
    "            S += abs(x[j + q] - x[j]) ** k\n",
    "            S /= delta - q + 1\n",
    "        H.append((np.log((sqrt_pi * S) / ((2 ** (k / 2)) * (sqrt_pi / 2) * (K**k)))) / (k * np.log(q / (len(x) - 1))))\n",
    "    return np.array(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_log_returns(target, window):\n",
    "    log_return_pipeline = LogReturnPipeline(StandardScaler())\n",
    "    x = pd.DataFrame(log_return_pipeline.preprocess(df, target))\n",
    "    x = x.rolling(window).mean().to_numpy().squeeze()\n",
    "    x = x[::window][1:]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_volumes(target, window):\n",
    "    x = np.log(1 + df[target])\n",
    "    x = x.rolling(window).mean().to_numpy().squeeze()\n",
    "    x = x[::window][1:]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCK = \"PEP\"\n",
    "WINDOW = 15\n",
    "\n",
    "avg_volumes = compute_avg_volumes([f\"volume_{STOCK}\"], window=WINDOW)\n",
    "avg_log_returns = compute_avg_log_returns([f\"mid_price_{STOCK}\"], window=WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder_exponent = compute_holder_exponent(df[f\"mid_price_{STOCK}\"].to_numpy(), delta=WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_log_returns.shape, avg_volumes.shape, holder_exponent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize=(5, 5))\n",
    "axs[0].plot(avg_log_returns)\n",
    "axs[1].plot(avg_volumes)\n",
    "axs[2].plot(holder_exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_avg_log_returns = np.random.randint(20, size=(4, 20))\n",
    "real_avg_volumes = np.random.randint(20, size=(4, 20))\n",
    "pred_avg_log_returns = np.random.randint(20, size=(4, 20))\n",
    "pred_avg_volumes = np.random.randint(20, size=(4, 20))\n",
    "stock_names = [\"a\", \"b\", \"c\", \"d\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(7 * 4, 10))\n",
    "\n",
    "for target_idx in range(4):\n",
    "    stock_name = stock_names[target_idx]\n",
    "\n",
    "    # Real volume-volatility correlation\n",
    "    title = f\"{stock_name} - Real\"\n",
    "    ax[0, target_idx].set_title(title)\n",
    "    ax[0, target_idx].scatter(\n",
    "        real_avg_log_returns[target_idx],\n",
    "        real_avg_volumes[target_idx],\n",
    "        color=\"C0\",\n",
    "    )\n",
    "    ax[0, target_idx].set_xlabel(\"Avg log-returns\")\n",
    "    ax[0, target_idx].set_ylabel(\"Avg log-volumes\")\n",
    "\n",
    "    # Pred volume-volatility correlation\n",
    "    title = f\"{stock_name} - Pred\"\n",
    "    ax[1, target_idx].set_title(title)\n",
    "    ax[1, target_idx].scatter(\n",
    "        pred_avg_log_returns[target_idx],\n",
    "        pred_avg_volumes[target_idx],\n",
    "        color=\"C1\",\n",
    "    )\n",
    "    ax[1, target_idx].set_xlabel(\"Avg log-returns\")\n",
    "    ax[1, target_idx].set_ylabel(\"Avg log-volumes\")\n",
    "\n",
    "fig.tight_layout()\n",
    "title = f\"Volume-Volatility Corr - Epoch {0}\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(avg_log_returns, avg_volumes, alpha=0.8, c=\"red\", edgecolors=\"none\", s=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(DATA, \"volumes_metrics\", \"*.csv\"))\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), axis=1, ignore_index=False)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWEEPNAME = \"stoic-jazz-65\"\n",
    "STOCK_NAMES = [\"PEP\", \"KO\", \"KSU\", \"NVDA\"]\n",
    "METRIC_NAMES = [\"Max\", \"Skew\", \"Min\", \"Mean\", \"Std\", \"Kurtosis\"]\n",
    "REAL_PRED = [\"Real\", \"Pred\"]\n",
    "COLUMNS = [\n",
    "    SWEEPNAME + \" - \" + realOpred + \" Volume: \" + metric_name + \"/\" + stock_name + \"_epoch\"\n",
    "    for stock_name in STOCK_NAMES\n",
    "    for metric_name in METRIC_NAMES\n",
    "    for realOpred in REAL_PRED\n",
    "]\n",
    "COLUMNS.insert(0, \"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df[COLUMNS].iloc[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv(os.path.join(DATA, \"volume_quality_metrics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_volume_corr_dist = pd.read_csv(os.path.join(DATA, \"avg_volume_corr_dist.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = [\n",
    "    \"KO_volume-KSU_volume_epoch\",\n",
    "    \"KO_volume-NVDA_volume_epoch\",\n",
    "    \"KO_volume-PEP_volume_epoch\",\n",
    "    \"NVDA_volume-KSU_volume_epoch\",\n",
    "    \"PEP_volume-KSU_volume_epoch\",\n",
    "    \"PEP_volume-NVDA_volume_epoch\",\n",
    "]\n",
    "COLUMNS = [SWEEPNAME + \" - corr_dist/\" + col for col in COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_volume_corr_dist[COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_volume_corr_dist[COLUMNS]\n",
    "df_avg_volume_corr_dist[\"avg\"] = df_avg_volume_corr_dist[COLUMNS].mean(axis=1)\n",
    "print(df_avg_volume_corr_dist[COLUMNS].iloc[65])\n",
    "print(df_avg_volume_corr_dist[[\"avg\"]].iloc[65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA + \"ohlc_KO_PEP_NVDA_KSU_train.csv\")\n",
    "df_val = pd.read_csv(DATA + \"ohlc_KO_PEP_NVDA_KSU_val.csv\")\n",
    "df = pd.concat([df_train, df_val])\n",
    "# print(df.columns)\n",
    "df = df.drop(\n",
    "    [\n",
    "        \"hour_slot\",\n",
    "        \"minute_slot\",\n",
    "        \"weekday\",\n",
    "        \"symbol\",\n",
    "        \"open_KO\",\n",
    "        \"high_KO\",\n",
    "        \"low_KO\",\n",
    "        \"norders_KO\",\n",
    "        \"mid_price_KO\",\n",
    "        \"open_PEP\",\n",
    "        \"high_PEP\",\n",
    "        \"low_PEP\",\n",
    "        \"norders_PEP\",\n",
    "        \"mid_price_PEP\",\n",
    "        \"open_NVDA\",\n",
    "        \"high_NVDA\",\n",
    "        \"low_NVDA\",\n",
    "        \"norders_NVDA\",\n",
    "        \"mid_price_NVDA\",\n",
    "        \"open_KSU\",\n",
    "        \"high_KSU\",\n",
    "        \"low_KSU\",\n",
    "        \"norders_KSU\",\n",
    "        \"mid_price_KSU\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "# print(df.columns)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_v = [\"volume_KO\", \"volume_PEP\", \"volume_NVDA\", \"volume_KSU\"]\n",
    "\n",
    "targets_p = [\"close_KO\", \"close_PEP\", \"close_NVDA\", \"close_KSU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[targets_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train[targets_p].corr(numeric_only=True)\n",
    "sb.heatmap(corr, cmap=\"Blues\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "\n",
    "def is_fitted(scaler: Union[MinMaxScaler, StandardScaler]) -> bool:\n",
    "    try:\n",
    "        check_is_fitted(scaler)\n",
    "        return True\n",
    "    except NotFittedError:\n",
    "        return False\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame, targets: List[str]) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def inverse_transform(self, x: np.ndarray, x_last: Optional[np.ndarray]) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__} (scaler={self.scaler})\"\n",
    "\n",
    "\n",
    "class ScalerPipeline(Pipeline):\n",
    "    def __init__(self, scaler: Union[MinMaxScaler, StandardScaler]) -> None:\n",
    "        super(ScalerPipeline, self).__init__()\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame, targets: List[str]) -> np.ndarray:\n",
    "        df_targets = df[targets]\n",
    "\n",
    "        if not is_fitted(self.scaler):\n",
    "            self.scaler.fit(df_targets)\n",
    "\n",
    "        return self.scaler.transform(df_targets)\n",
    "\n",
    "    def inverse_transform(self, x: np.ndarray, x_last: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        return self.scaler.inverse_transform(x)\n",
    "\n",
    "\n",
    "class LogReturnPipeline(Pipeline):\n",
    "    def __init__(self, scaler: Union[MinMaxScaler, StandardScaler]) -> None:\n",
    "        super(LogReturnPipeline, self).__init__()\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame, targets: List[str]) -> np.ndarray:\n",
    "        df_targets = df[targets]\n",
    "        log_returns = np.log(df_targets / df_targets.shift(1)).fillna(0).to_numpy()\n",
    "\n",
    "        if not is_fitted(self.scaler):\n",
    "            self.scaler.fit(log_returns)\n",
    "\n",
    "        return self.scaler.transform(log_returns)\n",
    "\n",
    "    def inverse_transform(self, x: np.ndarray, x_last: np.ndarray) -> np.ndarray:\n",
    "        log_returns = self.scaler.inverse_transform(x)\n",
    "        return x_last * (np.cumprod(np.exp(log_returns), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return_pipeline = LogReturnPipeline(StandardScaler())\n",
    "df_pre = log_return_pipeline.preprocess(df_train, targets_p)\n",
    "df_pre = pd.DataFrame(df_pre, columns=[\"KO\", \"PEP\", \"NVDA\", \"KSU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_pre.corr(numeric_only=True)\n",
    "sb.heatmap(corr, cmap=\"Blues\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_pipeline = ScalerPipeline(MinMaxScaler())\n",
    "df_pre = scaler_pipeline.preprocess(df_train, targets_v)\n",
    "df_pre = pd.DataFrame(df_pre, columns=[\"KO\", \"PEP\", \"NVDA\", \"KSU\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_pre.corr(numeric_only=True)\n",
    "sb.heatmap(corr, cmap=\"Blues\", annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
